{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoost Tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/HrSwBozOuZAvZTRhPOQM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alviyandriadillah/Machine-Learning/blob/main/XGBoost_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K23os46EVQh",
        "outputId": "3ed3bb93-b856-454e-caf2-f19afb6b0939"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkY92byEJGBt"
      },
      "source": [
        "We will start with the data pre-loaded into train_X, test_X, train_y, test_y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vmr3vo9Hw-H"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/XGBoost/train.csv')\n",
        "data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
        "y = data.SalePrice\n",
        "X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n",
        "train_X, test_X, train_y, test_y = train_test_split(X.values, y.values, test_size=0.25)\n",
        "\n",
        "my_imputer = SimpleImputer()\n",
        "train_X = my_imputer.fit_transform(train_X)\n",
        "test_X = my_imputer.transform(test_X)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlXzi-SMJMAs"
      },
      "source": [
        "We build and fit a model just as we would in scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvJOL1-PJA54",
        "outputId": "d617d3d4-aabb-4d1d-816a-d5a445a33e19"
      },
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "my_model = XGBRegressor()\n",
        "# Add silent=True to avoid printing out updates with each cycle\n",
        "my_model.fit(train_X, train_y, verbose=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[03:58:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
              "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
              "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
              "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "             silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwm2nP1cJldt"
      },
      "source": [
        "**Make Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oqKhDt3Jg9W",
        "outputId": "9f12f4cc-685c-408a-dfb2-dd0ac855185c"
      },
      "source": [
        "predictions = my_model.predict(test_X)\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error : 17338.770194777397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKQEiU-4JvmE"
      },
      "source": [
        "**Model Tuning**\n",
        "\n",
        "XGBoost has a few parameters that can dramatically affect your model's accuracy and training speed. The first parameters you should understand are:\n",
        "\n",
        "n_estimators and early_stopping_rounds\n",
        "n_estimators specifies how many times to go through the modeling cycle described above.\n",
        "\n",
        "In the underfitting vs overfitting graph, n_estimators moves you further to the right. Too low a value causes underfitting, which is inaccurate predictions on both training data and new data. Too large a value causes overfitting, which is accurate predictions on training data, but inaccurate predictions on new data (which is what we care about). You can experiment with your dataset to find the ideal. Typical values range from 100-1000, though this depends a lot on the learning rate discussed below.\n",
        "\n",
        "The argument early_stopping_rounds offers a way to automatically find the ideal value. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n",
        "\n",
        "Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. early_stopping_rounds = 5 is a reasonable value. Thus we stop after 5 straight rounds of deteriorating validation scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2JFKK7jJruV",
        "outputId": "485b3c5d-3f55-472a-f572-738c4deae7bf"
      },
      "source": [
        "my_model = XGBRegressor(n_estimators=1000)\n",
        "my_model.fit(train_X, train_y, early_stopping_rounds=5, \n",
        "             eval_set=[(test_X, test_y)], verbose=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04:00:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
              "             max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
              "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
              "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "             silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwq4PnykKJVW"
      },
      "source": [
        "**learning_rate**\n",
        "\n",
        "Here's a subtle but important trick for better XGBoost models:\n",
        "\n",
        "Instead of getting predictions by simply adding up the predictions from each component model, we will multiply the predictions from each model by a small number before adding them in. This means each tree we add to the ensemble helps us less. In practice, this reduces the model's propensity to overfit.\n",
        "\n",
        "So, you can use a higher value of n_estimators without overfitting. If you use early stopping, the appropriate number of trees will be set automatically.\n",
        "\n",
        "In general, a small learning rate (and large number of estimators) will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle.\n",
        "\n",
        "Modifying the example above to include a learing rate would yield the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8_SYqZVKZLO",
        "outputId": "83783e06-7518-4923-daf5-e76edfa10e01"
      },
      "source": [
        "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
        "my_model.fit(train_X, train_y, early_stopping_rounds=5, \n",
        "             eval_set=[(test_X, test_y)], verbose=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04:02:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "             importance_type='gain', learning_rate=0.05, max_delta_step=0,\n",
              "             max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
              "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
              "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "             silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}