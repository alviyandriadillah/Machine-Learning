{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A Beginner’s guide to XGBoost",
      "provenance": [],
      "authorship_tag": "ABX9TyNtqxSmZR69zq/pkfxDRojP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alviyandriadillah/Machine-Learning/blob/main/A_Beginner%E2%80%99s_guide_to_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tOiU9sahyE8"
      },
      "source": [
        "**Setting up our data with XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT8knqcHhQP5"
      },
      "source": [
        "from sklearn import datasets\n",
        "import xgboost as xgb\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBhH3VNthVNQ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mIaHvkMhXsM"
      },
      "source": [
        "D_train = xgb.DMatrix(X_train, label=Y_train)\n",
        "D_test = xgb.DMatrix(X_test, label=Y_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkdt75PKh4KU"
      },
      "source": [
        "**Defining an XGBoost model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL0YcPa9haXE"
      },
      "source": [
        "param = {\n",
        "    'eta': 0.3, \n",
        "    'max_depth': 3,  \n",
        "    'objective': 'multi:softprob',  \n",
        "    'num_class': 3} \n",
        "\n",
        "steps = 20  # The number of training iterations"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGHGfM8siCBW"
      },
      "source": [
        "**Training and Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmE1YbquhgU1"
      },
      "source": [
        "model = xgb.train(param, D_train, steps)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YsA2x7whhls",
        "outputId": "c87ff200-c76a-403b-ddb3-dc93936629bd"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "preds = model.predict(D_test)\n",
        "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
        "\n",
        "print(\"Precision = {}\".format(precision_score(Y_test, best_preds, average='macro')))\n",
        "print(\"Recall = {}\".format(recall_score(Y_test, best_preds, average='macro')))\n",
        "print(\"Accuracy = {}\".format(accuracy_score(Y_test, best_preds)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision = 0.9666666666666667\n",
            "Recall = 0.9629629629629629\n",
            "Accuracy = 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh2f0pvAhsVr"
      },
      "source": [
        "**Further Exploration with XGBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7pRlsJ_mFDq"
      },
      "source": [
        "that just about sums up the basics of XGBoost. But there are some more cool features that’ll help you get the most out of your models.\n",
        "\n",
        "*   The gamma parameter can also help with controlling overfitting. It specifies the minimum reduction in the loss required to make a further partition on a leaf node of the tree. I.e if creating a new node doesn’t reduce the loss by a certain amount, then we won’t create it at all.\n",
        "\n",
        "*   The booster parameter allows you to set the type of model you will use when building the ensemble. The default is gbtree which builds an ensemble of decision trees. If your data isn’t too complicated, you can go with the faster and simpler gblinear option which builds an ensemble of linear models.\n",
        "\n",
        "*   Setting the optimal hyperparameters of any ML model can be a challenge. So why not let Scikit Learn do it for you? We can combine Scikit Learn’s grid search with an XGBoost classifier quite easily:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "101dUc1AhtSE",
        "outputId": "e7ffd126-57d7-4d3c-b3b1-9d793e6061a5"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "clf = xgb.XGBClassifier()\n",
        "parameters = {\n",
        "     \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
        "     \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
        "     \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
        "     \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
        "     \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
        "     }\n",
        "\n",
        "grid = GridSearchCV(clf,\n",
        "                    parameters, n_jobs=4,\n",
        "                    scoring=\"neg_log_loss\",\n",
        "                    cv=3)\n",
        "\n",
        "grid.fit(X_train, Y_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
              "                                     colsample_bylevel=1, colsample_bynode=1,\n",
              "                                     colsample_bytree=1, gamma=0,\n",
              "                                     learning_rate=0.1, max_delta_step=0,\n",
              "                                     max_depth=3, min_child_weight=1,\n",
              "                                     missing=None, n_estimators=100, n_jobs=1,\n",
              "                                     nthread=None, objective='binary:logistic',\n",
              "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
              "                                     scale_po...ight=1, seed=None, silent=None,\n",
              "                                     subsample=1, verbosity=1),\n",
              "             iid='deprecated', n_jobs=4,\n",
              "             param_grid={'colsample_bytree': [0.3, 0.4, 0.5, 0.7],\n",
              "                         'eta': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
              "                         'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
              "                         'max_depth': [3, 4, 5, 6, 8, 10, 12, 15],\n",
              "                         'min_child_weight': [1, 3, 5, 7]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='neg_log_loss', verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcghHT87lK_z"
      },
      "source": [
        "Only do that on a big dataset if you have time to kill — doing a grid search is essentially training an ensemble of decision trees many times over!\n",
        "*   Once your XGBoost model is trained, you can dump a human readable description of it into a text file\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yukfYarWlBrz"
      },
      "source": [
        "model.dump_model('dump.raw.txt')"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}